# Paragon

**Parallel Architecture for Resilient Adaptive Growth & Optimized Networks**

Paragon is a parallel architecture designed for building resilient and adaptive neural networks. It aims to optimize network performance and scalability while providing a flexible framework for advanced AI research and development.

## Current Implementation

Paragon is actively developed and currently supports foundational neural network architectures with the following capabilities:

- Fully connected and locally connected layers with forward and backward propagation.
- Multithreading integrated in training loops with configurable CPU usage.
- Experimental support for discrete diffusion models.
- Basic transformer encoder with multi-head attention and positional encoding.

## Future Goals

Paragon is designed with ambitious scalability and performance enhancements in mind. The following features are planned:

- GPU and NPU acceleration for faster training and inference.
- Horizontal scaling for distributed training across multiple machines.
- Expanded neuron types beyond the current dense implementation.
- Enhanced multithreading for better performance on multi-core systems.
- WebAssembly integration for browser-based execution.
- Resilience mechanisms for fault tolerance and stability.
